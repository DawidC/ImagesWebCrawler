Spark Command: /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -cp /home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://192.168.56.101:7077
========================================
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/01/17 16:22:59 INFO Worker: Started daemon with process name: 1155@spark2
18/01/17 16:22:59 INFO SignalUtils: Registered signal handler for TERM
18/01/17 16:22:59 INFO SignalUtils: Registered signal handler for HUP
18/01/17 16:22:59 INFO SignalUtils: Registered signal handler for INT
18/01/17 16:22:59 WARN Utils: Your hostname, spark2 resolves to a loopback address: 127.0.1.1; using 192.168.56.102 instead (on interface enp0s3)
18/01/17 16:22:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/01/17 16:23:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/01/17 16:23:02 INFO SecurityManager: Changing view acls to: dawid
18/01/17 16:23:02 INFO SecurityManager: Changing modify acls to: dawid
18/01/17 16:23:02 INFO SecurityManager: Changing view acls groups to: 
18/01/17 16:23:02 INFO SecurityManager: Changing modify acls groups to: 
18/01/17 16:23:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/17 16:23:03 INFO Utils: Successfully started service 'sparkWorker' on port 45404.
18/01/17 16:23:04 INFO Worker: Starting Spark worker 192.168.56.102:45404 with 8 cores, 1024.0 MB RAM
18/01/17 16:23:04 INFO Worker: Running Spark version 2.2.1
18/01/17 16:23:04 INFO Worker: Spark home: /home/dawid/spark/spark-2.2.1-bin-hadoop2.7
18/01/17 16:23:04 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
18/01/17 16:23:05 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://192.168.56.102:8081
18/01/17 16:23:05 INFO Worker: Connecting to master 192.168.56.101:7077...
18/01/17 16:23:05 INFO TransportClientFactory: Successfully created connection to /192.168.56.101:7077 after 273 ms (0 ms spent in bootstraps)
18/01/17 16:23:06 INFO Worker: Successfully registered with master spark://192.168.56.101:7077
18/01/17 16:39:29 INFO Worker: Asked to launch executor app-20180117163928-0000/0 for PythonPi
18/01/17 16:39:29 INFO SecurityManager: Changing view acls to: dawid
18/01/17 16:39:29 INFO SecurityManager: Changing modify acls to: dawid
18/01/17 16:39:29 INFO SecurityManager: Changing view acls groups to: 
18/01/17 16:39:29 INFO SecurityManager: Changing modify acls groups to: 
18/01/17 16:39:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/17 16:39:29 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java" "-cp" "/home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=43534" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.56.102:43534" "--executor-id" "0" "--hostname" "192.168.56.102" "--cores" "8" "--app-id" "app-20180117163928-0000" "--worker-url" "spark://Worker@192.168.56.102:45404"
18/01/17 16:39:41 INFO Worker: Asked to kill executor app-20180117163928-0000/0
18/01/17 16:39:41 INFO ExecutorRunner: Runner thread for executor app-20180117163928-0000/0 interrupted
18/01/17 16:39:41 INFO ExecutorRunner: Killing process!
18/01/17 16:39:42 INFO Worker: Executor app-20180117163928-0000/0 finished with state KILLED exitStatus 143
18/01/17 16:39:42 INFO ExternalShuffleBlockResolver: Application app-20180117163928-0000 removed, cleanupLocalDirs = true
18/01/17 16:39:42 INFO Worker: Cleaning up local directories for application app-20180117163928-0000
18/01/17 16:56:46 INFO Worker: Asked to launch executor app-20180117165645-0001/0 for PythonWordCount
18/01/17 16:56:46 INFO SecurityManager: Changing view acls to: dawid
18/01/17 16:56:46 INFO SecurityManager: Changing modify acls to: dawid
18/01/17 16:56:46 INFO SecurityManager: Changing view acls groups to: 
18/01/17 16:56:46 INFO SecurityManager: Changing modify acls groups to: 
18/01/17 16:56:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/17 16:56:46 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java" "-cp" "/home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=39339" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.56.102:39339" "--executor-id" "0" "--hostname" "192.168.56.102" "--cores" "8" "--app-id" "app-20180117165645-0001" "--worker-url" "spark://Worker@192.168.56.102:45404"
18/01/17 16:56:53 INFO Worker: Asked to kill executor app-20180117165645-0001/0
18/01/17 16:56:53 INFO ExecutorRunner: Runner thread for executor app-20180117165645-0001/0 interrupted
18/01/17 16:56:53 INFO ExecutorRunner: Killing process!
18/01/17 16:56:53 INFO Worker: Executor app-20180117165645-0001/0 finished with state KILLED exitStatus 143
18/01/17 16:56:53 INFO ExternalShuffleBlockResolver: Application app-20180117165645-0001 removed, cleanupLocalDirs = true
18/01/17 16:56:53 INFO Worker: Cleaning up local directories for application app-20180117165645-0001
18/01/17 17:00:56 INFO Worker: Asked to launch executor app-20180117170056-0002/0 for PythonWordCount
18/01/17 17:00:56 INFO SecurityManager: Changing view acls to: dawid
18/01/17 17:00:56 INFO SecurityManager: Changing modify acls to: dawid
18/01/17 17:00:56 INFO SecurityManager: Changing view acls groups to: 
18/01/17 17:00:56 INFO SecurityManager: Changing modify acls groups to: 
18/01/17 17:00:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/17 17:00:56 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java" "-cp" "/home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=37152" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.56.102:37152" "--executor-id" "0" "--hostname" "192.168.56.102" "--cores" "8" "--app-id" "app-20180117170056-0002" "--worker-url" "spark://Worker@192.168.56.102:45404"
18/01/17 17:01:01 INFO Worker: Asked to kill executor app-20180117170056-0002/0
18/01/17 17:01:01 INFO ExecutorRunner: Runner thread for executor app-20180117170056-0002/0 interrupted
18/01/17 17:01:01 INFO ExecutorRunner: Killing process!
18/01/17 17:01:01 INFO Worker: Executor app-20180117170056-0002/0 finished with state KILLED exitStatus 143
18/01/17 17:01:01 INFO ExternalShuffleBlockResolver: Application app-20180117170056-0002 removed, cleanupLocalDirs = true
18/01/17 17:01:01 INFO Worker: Cleaning up local directories for application app-20180117170056-0002
18/01/17 17:02:32 INFO Worker: Asked to launch executor app-20180117170232-0003/0 for PythonWordCount
18/01/17 17:02:32 INFO SecurityManager: Changing view acls to: dawid
18/01/17 17:02:32 INFO SecurityManager: Changing modify acls to: dawid
18/01/17 17:02:32 INFO SecurityManager: Changing view acls groups to: 
18/01/17 17:02:32 INFO SecurityManager: Changing modify acls groups to: 
18/01/17 17:02:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/17 17:02:32 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java" "-cp" "/home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=36695" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.56.102:36695" "--executor-id" "0" "--hostname" "192.168.56.102" "--cores" "8" "--app-id" "app-20180117170232-0003" "--worker-url" "spark://Worker@192.168.56.102:45404"
18/01/17 17:02:49 INFO Worker: Asked to kill executor app-20180117170232-0003/0
18/01/17 17:02:49 INFO ExecutorRunner: Runner thread for executor app-20180117170232-0003/0 interrupted
18/01/17 17:02:49 INFO ExecutorRunner: Killing process!
18/01/17 17:02:49 INFO Worker: Executor app-20180117170232-0003/0 finished with state KILLED exitStatus 143
18/01/17 17:02:49 INFO Worker: Cleaning up local directories for application app-20180117170232-0003
18/01/17 17:02:49 INFO ExternalShuffleBlockResolver: Application app-20180117170232-0003 removed, cleanupLocalDirs = true
18/01/17 17:12:11 INFO Worker: Asked to launch executor app-20180117171211-0004/0 for PythonWordCount
18/01/17 17:12:11 INFO SecurityManager: Changing view acls to: dawid
18/01/17 17:12:11 INFO SecurityManager: Changing modify acls to: dawid
18/01/17 17:12:11 INFO SecurityManager: Changing view acls groups to: 
18/01/17 17:12:11 INFO SecurityManager: Changing modify acls groups to: 
18/01/17 17:12:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/17 17:12:11 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java" "-cp" "/home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=44802" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.56.102:44802" "--executor-id" "0" "--hostname" "192.168.56.102" "--cores" "8" "--app-id" "app-20180117171211-0004" "--worker-url" "spark://Worker@192.168.56.102:45404"
18/01/17 17:12:28 INFO Worker: Asked to kill executor app-20180117171211-0004/0
18/01/17 17:12:28 INFO ExecutorRunner: Runner thread for executor app-20180117171211-0004/0 interrupted
18/01/17 17:12:28 INFO ExecutorRunner: Killing process!
18/01/17 17:12:28 INFO Worker: Executor app-20180117171211-0004/0 finished with state KILLED exitStatus 143
18/01/17 17:12:28 INFO ExternalShuffleBlockResolver: Application app-20180117171211-0004 removed, cleanupLocalDirs = true
18/01/17 17:12:28 INFO Worker: Cleaning up local directories for application app-20180117171211-0004
18/01/17 17:13:03 INFO Worker: Asked to launch executor app-20180117171303-0005/0 for PythonWordCount
18/01/17 17:13:03 INFO SecurityManager: Changing view acls to: dawid
18/01/17 17:13:03 INFO SecurityManager: Changing modify acls to: dawid
18/01/17 17:13:03 INFO SecurityManager: Changing view acls groups to: 
18/01/17 17:13:03 INFO SecurityManager: Changing modify acls groups to: 
18/01/17 17:13:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/17 17:13:03 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java" "-cp" "/home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=42553" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.56.102:42553" "--executor-id" "0" "--hostname" "192.168.56.102" "--cores" "8" "--app-id" "app-20180117171303-0005" "--worker-url" "spark://Worker@192.168.56.102:45404"
18/01/17 17:13:21 INFO Worker: Asked to kill executor app-20180117171303-0005/0
18/01/17 17:13:21 INFO ExecutorRunner: Runner thread for executor app-20180117171303-0005/0 interrupted
18/01/17 17:13:21 INFO ExecutorRunner: Killing process!
18/01/17 17:13:22 INFO Worker: Executor app-20180117171303-0005/0 finished with state KILLED exitStatus 143
18/01/17 17:13:22 INFO Worker: Cleaning up local directories for application app-20180117171303-0005
18/01/17 17:13:22 INFO ExternalShuffleBlockResolver: Application app-20180117171303-0005 removed, cleanupLocalDirs = true
18/01/18 00:17:39 INFO Worker: Asked to launch executor app-20180118001739-0006/0 for PythonWordCount
18/01/18 00:17:39 INFO SecurityManager: Changing view acls to: dawid
18/01/18 00:17:39 INFO SecurityManager: Changing modify acls to: dawid
18/01/18 00:17:39 INFO SecurityManager: Changing view acls groups to: 
18/01/18 00:17:39 INFO SecurityManager: Changing modify acls groups to: 
18/01/18 00:17:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dawid); groups with view permissions: Set(); users  with modify permissions: Set(dawid); groups with modify permissions: Set()
18/01/18 00:17:39 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java" "-cp" "/home/dawid/spark/spark-2.2.1-bin-hadoop2.7//conf/:/home/dawid/spark/spark-2.2.1-bin-hadoop2.7/jars/*" "-Xmx1024M" "-Dspark.driver.port=45375" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.56.102:45375" "--executor-id" "0" "--hostname" "192.168.56.102" "--cores" "8" "--app-id" "app-20180118001739-0006" "--worker-url" "spark://Worker@192.168.56.102:45404"
18/01/18 00:17:56 INFO Worker: Asked to kill executor app-20180118001739-0006/0
18/01/18 00:17:56 INFO ExecutorRunner: Runner thread for executor app-20180118001739-0006/0 interrupted
18/01/18 00:17:56 INFO ExecutorRunner: Killing process!
18/01/18 00:17:56 INFO Worker: Executor app-20180118001739-0006/0 finished with state KILLED exitStatus 143
18/01/18 00:17:56 INFO ExternalShuffleBlockResolver: Application app-20180118001739-0006 removed, cleanupLocalDirs = true
18/01/18 00:17:56 INFO Worker: Cleaning up local directories for application app-20180118001739-0006
18/01/18 01:01:08 INFO Worker: 192.168.56.101:7077 Disassociated !
18/01/18 01:01:08 ERROR Worker: Connection to master failed! Waiting for master to reconnect...
18/01/18 01:01:08 INFO Worker: 192.168.56.101:7077 Disassociated !
18/01/18 01:01:08 ERROR Worker: Connection to master failed! Waiting for master to reconnect...
18/01/18 01:01:08 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already.
18/01/18 01:01:08 INFO Worker: Connecting to master 192.168.56.101:7077...
18/01/18 01:01:08 INFO TransportClientFactory: Found inactive connection to /192.168.56.101:7077, creating a new one.
18/01/18 01:01:08 WARN Worker: Failed to connect to master 192.168.56.101:7077
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:100)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:108)
	at org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1.run(Worker.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to /192.168.56.101:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	... 4 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /192.168.56.101:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
18/01/18 01:01:14 INFO Worker: Retrying connection to master (attempt # 1)
18/01/18 01:01:14 INFO Worker: Connecting to master 192.168.56.101:7077...
18/01/18 01:01:14 INFO TransportClientFactory: Found inactive connection to /192.168.56.101:7077, creating a new one.
18/01/18 01:01:15 ERROR Worker: RECEIVED SIGNAL TERM
18/01/18 01:01:15 INFO ShutdownHookManager: Shutdown hook called
18/01/18 01:01:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-164436d3-727b-4cef-b82b-1602d90bc3f7
